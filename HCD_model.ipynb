{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clyde2020/ML_Portfolio/blob/main/HCD_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "eZGb3Orodx0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FWddduZzMLCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fa7667-f261-44f1-8ac6-758329d81b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.3 MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "!pip install keras-tuner -q\n",
        "from keras_tuner import RandomSearch\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from keras.models import load_model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNEX1tDP1_6V"
      },
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jRAqy2RZMOd3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Histopath/train_labels.csv')\n",
        "\n",
        "def append_ext(file):\n",
        "  return file + '.tif'\n",
        "\n",
        "def remove_ext(file):\n",
        "  return file.split('.')[0]\n",
        "\n",
        "# Define the folder to pull images from - \n",
        "# Data split is faster with smaller source folder\n",
        "source_folder = '/content/gdrive/MyDrive/Histopath/120k_set'\n",
        "\n",
        "# It takes awhile for this list to load, so I pickled it for convenience\n",
        "with open('/content/gdrive/MyDrive/Histopath/Saved_data/120k_tif_source_list', 'rb') as f:\n",
        "  tif_source_list = pickle.load(f)\n",
        "\n",
        "state = 60\n",
        "home_dir = '/content/gdrive/MyDrive/Histopath/Large_96K_total1'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the below commented out code if we don't use pickled data"
      ],
      "metadata": {
        "id": "PhKO3aZSfW2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tif_source_list = os.listdir(source_folder)"
      ],
      "metadata": {
        "id": "9rXTXs351NZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/MyDrive/Histopath/Saved_data/120k_tif_source_list', 'wb') as f:\n",
        "#   pickle.dump(tif_source_list, f)"
      ],
      "metadata": {
        "id": "to2y5f7JDXRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process data"
      ],
      "metadata": {
        "id": "BjWXS8h_QJ5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_list = map(remove_ext, tif_source_list)\n",
        "df_sample = df[df['id'].isin(source_list)]\n",
        "picdict = dict(zip(df_sample['id'], df_sample['label']))"
      ],
      "metadata": {
        "id": "UjqvqUcVzHwl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many of each class are represented?"
      ],
      "metadata": {
        "id": "IPyB2VL1kP3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6g95Z54DEs",
        "outputId": "4b6aa164-ad44-4dfd-8297-e33705d24c45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    71573\n",
              "1    48427\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 48000  # Update for train\n",
        "\n",
        "# Sample a number of each label type\n",
        "df_0_train = df_sample[df_sample['label'] == 0].sample(sample_size)\n",
        "df_1_train = df_sample[df_sample['label'] == 1].sample(sample_size)\n",
        "\n",
        "# Put into single dataframe\n",
        "df_train_full = pd.concat([df_0_train, df_1_train], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Add the extension\n",
        "df_train_full['id'] = df_train_full['id'].apply(append_ext)\n",
        "\n",
        "# Shuffle the dataframe\n",
        "df_train_full = shuffle(df_train_full)"
      ],
      "metadata": {
        "id": "ZH50QMB41BDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, valid split lists\n",
        "df_train, df_valid = train_test_split(df_train_full, \n",
        "                                      test_size=10000, \n",
        "                                      random_state=state, \n",
        "                                      stratify=df_train_full['label'])\n",
        "train_pre_list = list(df_train['id'])\n",
        "valid_pre_list = list(df_valid['id'])\n",
        "\n",
        "# Get id lists\n",
        "df_source = pd.DataFrame(tif_source_list, columns=['id'])\n",
        "df_train_list = df_source[df_source.id.isin(train_pre_list)]\n",
        "df_valid_list = df_source[df_source.id.isin(valid_pre_list)]\n",
        "\n",
        "train_list = list(df_train_list['id'])\n",
        "valid_list = list(df_valid_list['id'])\n",
        "\n",
        "# Make sure lists have correct # of elements\n",
        "print(len(train_list), len(valid_list))\n",
        "assert (len(train_list) + len(valid_list)) == (2 * sample_size)"
      ],
      "metadata": {
        "id": "CvoKmu8TwKHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8037647f-bacc-4d16-b109-0a3de7a92c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFOJukYV3TO1"
      },
      "source": [
        "Set data folder variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vphGaDh-v6Yw"
      },
      "outputs": [],
      "source": [
        "# Update these variables according to the dataset to use\n",
        "train_comp = '{}/Train'.format(home_dir)\n",
        "valid_comp = '{}/Valid'.format(home_dir)\n",
        "benign_train_comp = '{}/Benign'.format(train_comp)\n",
        "benign_valid_comp = '{}/Benign'.format(valid_comp)\n",
        "mal_train_comp = '{}/Malignant'.format(train_comp)\n",
        "mal_valid_comp = '{}/Malignant'.format(valid_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJ9Uh6O2IS6"
      },
      "source": [
        "Folder creation before split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKf1M2BrMsAJ"
      },
      "outputs": [],
      "source": [
        "os.mkdir(home_dir)\n",
        "os.mkdir(train_comp)\n",
        "os.mkdir(valid_comp)\n",
        "os.mkdir(benign_train_comp)\n",
        "os.mkdir(benign_valid_comp)\n",
        "os.mkdir(mal_train_comp)\n",
        "os.mkdir(mal_valid_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfZhHyw4f3p"
      },
      "source": [
        "This function splits the dataset into malignant and benign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swhsUAPlNieq"
      },
      "outputs": [],
      "source": [
        "def split_data(LIST, SOURCE, MAL_FOLDER, BENIGN_FOLDER):\n",
        "  for fname in LIST:\n",
        "    origin = os.path.join(SOURCE, fname)\n",
        "    name = remove_ext(fname)\n",
        "    if picdict[name] == 0:\n",
        "      shutil.copy(src=origin, dst=BENIGN_FOLDER)\n",
        "    else:\n",
        "      shutil.copy(src=origin, dst=MAL_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the data split"
      ],
      "metadata": {
        "id": "XMFfM50oGsYn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK55HVDWdGOL"
      },
      "outputs": [],
      "source": [
        "split_data(valid_list, source_folder, mal_valid_comp, benign_valid_comp)\n",
        "split_data(train_list, source_folder, mal_train_comp, benign_train_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxH6v3Yq1SKx"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0no8J-9nP_SB",
        "outputId": "5e765ff2-4cb0-487b-db57-61ee9ddb6d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 86400 images belonging to 2 classes.\n",
            "Found 9600 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.0,\n",
        "                                  rotation_range=45,\n",
        "                                  width_shift_range=0.2,\n",
        "                                  height_shift_range=0.2,\n",
        "                                  shear_range=0.2,\n",
        "                                  horizontal_flip=True,\n",
        "                                  fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255.0)  \n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    valid_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and execute the model"
      ],
      "metadata": {
        "id": "fvwQgw1vROUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = 75\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(state)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=32, \n",
        "                            kernel_size=3, \n",
        "                            input_shape=(96, 96, 3), \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=32, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(filters=64, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=64, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(filters=128, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=128, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc']\n",
        "              )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='acc',\n",
        "                              factor=0.5,\n",
        "                              patience=1,\n",
        "                              cooldown=2,\n",
        "                              min_lr=1e-4\n",
        "                              )\n",
        "\n",
        "mc = ModelCheckpoint(monitor='val_acc', \n",
        "                     filepath='/content/gdrive/MyDrive/Histopath/Saved_models/Large_96k_Reduce_LR_6Layer_bestmodel.h5', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True, \n",
        "                     mode='max'\n",
        "                     )\n",
        "\n",
        "cd = [mc, reduce_lr]"
      ],
      "metadata": {
        "id": "T_yFUFEPu3fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    epochs=24,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=cd\n",
        "                    )"
      ],
      "metadata": {
        "id": "4_UnnKZCu3iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Predictions and create Submission CSV file"
      ],
      "metadata": {
        "id": "T303lGzO6xVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/gdrive/MyDrive/Histopath/Saved_models/Large_96k_Reduce_LR_6Layer_bestmodel.h5')\n",
        "y_pred = pd.read_csv('/content/gdrive/MyDrive/Histopath/blank_sample_submission.csv')\n",
        "test_comp = '/content/gdrive/MyDrive/Histopath/test'\n",
        "y_pred['id'] = y_pred['id'].apply(append_ext)"
      ],
      "metadata": {
        "id": "Z0X1UBS5o_Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_files = pd.DataFrame({'file_name': os.listdir(test_comp)})\n",
        "# with open('/content/gdrive/MyDrive/Histopath/full_test_files', 'wb') as f:\n",
        "#   pickle.dump(test_files, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Histopath/Saved_data/full_test_files', 'rb') as f:\n",
        "  test_files = pickle.load(f)"
      ],
      "metadata": {
        "id": "rZIHm08wMlC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1/255.0)  \n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_files,\n",
        "    directory=test_comp,\n",
        "    x_col='file_name',\n",
        "    target_size=(96, 96),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcFq7lNbpapn",
        "outputId": "8ec59007-141a-48e6-aedc-57abe34a64ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57458 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_test = len(test_files)\n",
        "len_test"
      ],
      "metadata": {
        "id": "hC8mdSJxycED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f233df-1314-4096-cfba-78ce424758a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57458"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_generator,\n",
        "                      steps=len_test,\n",
        "                      verbose=1)"
      ],
      "metadata": {
        "id": "Hti5H1Elsxam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81402eb8-5b7f-435e-e514-e479ac9cc6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57458/57458 [==============================] - 9576s 167ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the prediction labels"
      ],
      "metadata": {
        "id": "_dY6zFyA6Chs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = preds[:, 0]\n",
        "preds = np.round(preds)\n",
        "preds.astype(int)"
      ],
      "metadata": {
        "id": "z-KhDlyYAol9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01b7948-38e2-4adc-9d12-31e4dd13d154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the prediction labels into the submission dataframe"
      ],
      "metadata": {
        "id": "Bsw2eSvQ6QTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_names = test_generator.filenames\n",
        "df_preds = pd.DataFrame({'id': test_file_names, 'label': preds})\n",
        "y_pred = y_pred.merge(df_preds, on='id')"
      ],
      "metadata": {
        "id": "cAu43Go4Bq8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get submission dataframe into correct format"
      ],
      "metadata": {
        "id": "T1bp68f7lT26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred['label'] = y_pred['label_y']\n",
        "y_pred.drop(['label_x'], axis=1, inplace=True)\n",
        "y_pred.drop(['label_y'], axis=1, inplace=True)\n",
        "y_pred['id'] = y_pred['id'].apply(remove_ext)"
      ],
      "metadata": {
        "id": "ZMnsdxJDFYvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.to_csv('/content/gdrive/MyDrive/Histopath/sample_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "bJLu8KQVHTak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get AUC"
      ],
      "metadata": {
        "id": "4UYCKIRHrksc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_generator = validation_datagen.flow_from_directory(\n",
        "    valid_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=1,\n",
        "    class_mode='binary',\n",
        "    shuffle=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwPqoAVigRPj",
        "outputId": "a107ac72-8f9c-476a-90b9-10e9defe592d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_val_test = len(val_test_generator)\n",
        "\n",
        "val_test_pred = model.predict(val_test_generator, \n",
        "                         steps=len_val_test,\n",
        "                         verbose=1)[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlIbNP74Tg27",
        "outputId": "b742dcad-d921-4a06-84b6-31491684c89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000/3000 [==============================] - 20s 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_true = val_test_generator.classes\n",
        "val_test_pred = np.round(val_test_pred)"
      ],
      "metadata": {
        "id": "-7mIDdA_T04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_pred.astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrjf8J7bWZp7",
        "outputId": "c53d25f2-b7e4-4037-af72-e728105157db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_val_test = np.equal(val_test_pred, val_test_true).sum() / len(val_test_pred)\n",
        "print('Validation accuracy: {:.3f}'.format(acc_val_test))"
      ],
      "metadata": {
        "id": "gMNeUgqlW3Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(val_test_true, val_test_pred)"
      ],
      "metadata": {
        "id": "uTpPxroJZrUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_val_test = auc(fpr, tpr)\n",
        "print('Validation AUC: {:.3f}'.format(auc_val_test))"
      ],
      "metadata": {
        "id": "e0E5wLBFiG95"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eZGb3Orodx0t",
        "TNEX1tDP1_6V",
        "BjWXS8h_QJ5R",
        "fvwQgw1vROUg",
        "T303lGzO6xVi",
        "4UYCKIRHrksc"
      ],
      "name": "HCD model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOlMe8dqCRiFXlbJg6TL3n2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}