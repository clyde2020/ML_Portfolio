{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clyde2020/ML_Portfolio/blob/main/Histopathologic_Cancer_Detection/HCD_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "eZGb3Orodx0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWddduZzMLCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbf49e8-0058-4eb7-f6d0-76534b5e0b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.7 MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "!pip install keras-tuner -q\n",
        "from keras_tuner import RandomSearch\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from keras.models import load_model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNEX1tDP1_6V"
      },
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRAqy2RZMOd3"
      },
      "outputs": [],
      "source": [
        "initial_dir = '/content/gdrive/MyDrive/Histopath'  # Mandatory update\n",
        "\n",
        "def append_ext(file):\n",
        "  return file + '.tif'\n",
        "\n",
        "def remove_ext(file):\n",
        "  return file.split('.')[0]\n",
        "\n",
        "df = pd.read_csv('{}/train_labels.csv'.format(initial_dir))\n",
        "state = 60\n",
        "target_size = (96, 96)\n",
        "class_mode = 'binary'\n",
        "source_folder = '{}/train'.format(initial_dir)\n",
        "home_dir = '{}/Full_set'.format(initial_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tif_source_list = os.listdir(source_folder)"
      ],
      "metadata": {
        "id": "9rXTXs351NZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View image"
      ],
      "metadata": {
        "id": "77rNcvFU5uaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image(path):\n",
        "  im_bgr = cv2.imread(path)\n",
        "  im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
        "  return im_rgb"
      ],
      "metadata": {
        "id": "_ZRHDgFL5wS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/Satellite/data/desert/desert(58).jpg'  # Mandatory update\n",
        "new_image = get_image(path=path)\n",
        "plt.imshow(new_image)\n",
        "plt.show()\n",
        "print(new_image.shape)"
      ],
      "metadata": {
        "id": "79icKJ9x5zBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process data"
      ],
      "metadata": {
        "id": "BjWXS8h_QJ5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_list = map(remove_ext, tif_source_list)\n",
        "picdict = dict(zip(df['id'], df['label']))"
      ],
      "metadata": {
        "id": "UjqvqUcVzHwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many of each class are represented?"
      ],
      "metadata": {
        "id": "IPyB2VL1kP3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6g95Z54DEs",
        "outputId": "34a3c637-438a-4ed2-fb53-093b5942a6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    130908\n",
              "1     89117\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 80000 \n",
        "\n",
        "# Sample a number of each label type\n",
        "df_0_train = df[df['label'] == 0].sample(sample_size)\n",
        "df_1_train = df[df['label'] == 1].sample(sample_size)\n",
        "\n",
        "# Put into single dataframe\n",
        "df_train_full = pd.concat([df_0_train, df_1_train], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Add the extension\n",
        "df_train_full['id'] = df_train_full['id'].apply(append_ext)\n",
        "\n",
        "# Shuffle the dataframe\n",
        "df_train_full = shuffle(df_train_full)"
      ],
      "metadata": {
        "id": "ZH50QMB41BDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, valid split lists\n",
        "df_train, df_valid = train_test_split(df_train_full, \n",
        "                                      test_size=10000, \n",
        "                                      random_state=state, \n",
        "                                      stratify=df_train_full['label'])\n",
        "train_list = list(df_train['id'])\n",
        "valid_list = list(df_valid['id'])\n",
        "# train_pre_list = list(df_train['id'])\n",
        "# valid_pre_list = list(df_valid['id'])\n",
        "\n",
        "# Get id lists\n",
        "# df_source = pd.DataFrame(tif_source_list, columns=['id'])\n",
        "# df_train_list = df_source[df_source.id.isin(train_pre_list)]\n",
        "# df_valid_list = df_source[df_source.id.isin(valid_pre_list)]\n",
        "\n",
        "# train_list = list(df_train_list['id'])\n",
        "# valid_list = list(df_valid_list['id'])\n",
        "\n",
        "# Make sure lists have correct # of elements\n",
        "print(len(train_list), len(valid_list))\n",
        "assert (len(train_list) + len(valid_list)) == (2 * sample_size)"
      ],
      "metadata": {
        "id": "CvoKmu8TwKHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab467d6-13df-4663-a9e9-ae88331f0998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFOJukYV3TO1"
      },
      "source": [
        "Set data folder variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vphGaDh-v6Yw"
      },
      "outputs": [],
      "source": [
        "# Update these variables according to the dataset to use\n",
        "train_comp = '{}/Train'.format(home_dir)\n",
        "valid_comp = '{}/Valid'.format(home_dir)\n",
        "benign_train_comp = '{}/Benign'.format(train_comp)\n",
        "benign_valid_comp = '{}/Benign'.format(valid_comp)\n",
        "mal_train_comp = '{}/Malignant'.format(train_comp)\n",
        "mal_valid_comp = '{}/Malignant'.format(valid_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJ9Uh6O2IS6"
      },
      "source": [
        "Folder creation before split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKf1M2BrMsAJ"
      },
      "outputs": [],
      "source": [
        "os.mkdir(home_dir)\n",
        "os.mkdir(train_comp)\n",
        "os.mkdir(valid_comp)\n",
        "os.mkdir(benign_train_comp)\n",
        "os.mkdir(benign_valid_comp)\n",
        "os.mkdir(mal_train_comp)\n",
        "os.mkdir(mal_valid_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfZhHyw4f3p"
      },
      "source": [
        "This function splits the dataset into malignant and benign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swhsUAPlNieq"
      },
      "outputs": [],
      "source": [
        "def split_data(LIST, SOURCE, MAL_FOLDER, BENIGN_FOLDER):\n",
        "  for fname in LIST:\n",
        "    origin = '{}/{}'.format(SOURCE, fname)\n",
        "    name = remove_ext(fname)\n",
        "    if picdict[name] == 0:\n",
        "      shutil.copy(src=origin, dst=BENIGN_FOLDER)\n",
        "    else:\n",
        "      shutil.copy(src=origin, dst=MAL_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the data split"
      ],
      "metadata": {
        "id": "XMFfM50oGsYn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK55HVDWdGOL"
      },
      "outputs": [],
      "source": [
        "split_data(LIST=valid_list, SOURCE=source_folder, MAL_FOLDER=mal_valid_comp, BENIGN_FOLDER=benign_valid_comp)\n",
        "split_data(LIST=train_list, SOURCE=source_folder, MAL_FOLDER=mal_train_comp, BENIGN_FOLDER=benign_train_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file was found to be corrupt during training"
      ],
      "metadata": {
        "id": "zSuqWFT0I3zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_file = f'{mal_train_comp}/cd0e59e19393ad3545664a31b149f15ef2f909c2.tif'\n",
        "os.remove(bad_file)"
      ],
      "metadata": {
        "id": "cTO8d_C_4h7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxH6v3Yq1SKx"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0no8J-9nP_SB"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.0,\n",
        "                                  rotation_range=45,\n",
        "                                  width_shift_range=0.2,\n",
        "                                  height_shift_range=0.2,\n",
        "                                  shear_range=0.2,\n",
        "                                  horizontal_flip=True,\n",
        "                                  fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255.0)  \n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    valid_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and execute the model"
      ],
      "metadata": {
        "id": "fvwQgw1vROUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = 75\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(state)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=32, \n",
        "                            kernel_size=3, \n",
        "                            input_shape=(96, 96, 3), \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=32, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(filters=64, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=64, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(filters=128, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=128, \n",
        "                            kernel_size=3, \n",
        "                            activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc']\n",
        "              )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='acc',\n",
        "                              factor=0.5,\n",
        "                              patience=1,\n",
        "                              cooldown=2,\n",
        "                              min_lr=1e-4\n",
        "                              )\n",
        "\n",
        "mc = ModelCheckpoint(monitor='val_acc', \n",
        "                     filepath='/content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True, \n",
        "                     mode='max'\n",
        "                     )\n",
        "\n",
        "cd = [mc, reduce_lr]"
      ],
      "metadata": {
        "id": "T_yFUFEPu3fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=cd\n",
        "                    )"
      ],
      "metadata": {
        "id": "4_UnnKZCu3iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82cb1a8a-704b-458a-afb1-be06cc4dce98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.4019 - acc: 0.8224\n",
            "Epoch 00001: val_acc improved from -inf to 0.66470, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 29508s 6s/step - loss: 0.4019 - acc: 0.8224 - val_loss: 1.0687 - val_acc: 0.6647 - lr: 0.0050\n",
            "Epoch 2/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.3322 - acc: 0.8580\n",
            "Epoch 00002: val_acc improved from 0.66470 to 0.69300, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1589s 339ms/step - loss: 0.3322 - acc: 0.8580 - val_loss: 0.8684 - val_acc: 0.6930 - lr: 0.0050\n",
            "Epoch 3/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2992 - acc: 0.8759\n",
            "Epoch 00003: val_acc did not improve from 0.69300\n",
            "4688/4688 [==============================] - 1444s 308ms/step - loss: 0.2992 - acc: 0.8759 - val_loss: 1.0815 - val_acc: 0.6594 - lr: 0.0050\n",
            "Epoch 4/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2733 - acc: 0.8887\n",
            "Epoch 00004: val_acc improved from 0.69300 to 0.73830, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1447s 309ms/step - loss: 0.2733 - acc: 0.8887 - val_loss: 0.8179 - val_acc: 0.7383 - lr: 0.0050\n",
            "Epoch 5/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2534 - acc: 0.8986\n",
            "Epoch 00005: val_acc improved from 0.73830 to 0.84750, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1443s 308ms/step - loss: 0.2534 - acc: 0.8986 - val_loss: 0.3820 - val_acc: 0.8475 - lr: 0.0050\n",
            "Epoch 6/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2394 - acc: 0.9049\n",
            "Epoch 00006: val_acc did not improve from 0.84750\n",
            "4688/4688 [==============================] - 1466s 313ms/step - loss: 0.2394 - acc: 0.9049 - val_loss: 0.7415 - val_acc: 0.8035 - lr: 0.0050\n",
            "Epoch 7/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2335 - acc: 0.9075\n",
            "Epoch 00007: val_acc improved from 0.84750 to 0.88590, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1574s 336ms/step - loss: 0.2335 - acc: 0.9075 - val_loss: 0.3263 - val_acc: 0.8859 - lr: 0.0050\n",
            "Epoch 8/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2254 - acc: 0.9124\n",
            "Epoch 00008: val_acc improved from 0.88590 to 0.90170, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1433s 306ms/step - loss: 0.2254 - acc: 0.9124 - val_loss: 0.2397 - val_acc: 0.9017 - lr: 0.0050\n",
            "Epoch 9/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2193 - acc: 0.9141\n",
            "Epoch 00009: val_acc did not improve from 0.90170\n",
            "4688/4688 [==============================] - 1438s 307ms/step - loss: 0.2193 - acc: 0.9141 - val_loss: 0.4742 - val_acc: 0.7954 - lr: 0.0050\n",
            "Epoch 10/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2140 - acc: 0.9165\n",
            "Epoch 00010: val_acc did not improve from 0.90170\n",
            "4688/4688 [==============================] - 1426s 304ms/step - loss: 0.2140 - acc: 0.9165 - val_loss: 0.3859 - val_acc: 0.8573 - lr: 0.0050\n",
            "Epoch 11/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2107 - acc: 0.9182\n",
            "Epoch 00011: val_acc did not improve from 0.90170\n",
            "4688/4688 [==============================] - 1418s 303ms/step - loss: 0.2107 - acc: 0.9182 - val_loss: 0.6308 - val_acc: 0.7809 - lr: 0.0050\n",
            "Epoch 12/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2078 - acc: 0.9194\n",
            "Epoch 00012: val_acc improved from 0.90170 to 0.91040, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1422s 303ms/step - loss: 0.2078 - acc: 0.9194 - val_loss: 0.2422 - val_acc: 0.9104 - lr: 0.0050\n",
            "Epoch 13/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.2036 - acc: 0.9207\n",
            "Epoch 00013: val_acc did not improve from 0.91040\n",
            "4688/4688 [==============================] - 1418s 302ms/step - loss: 0.2036 - acc: 0.9207 - val_loss: 0.2514 - val_acc: 0.8994 - lr: 0.0050\n",
            "Epoch 14/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.1997 - acc: 0.9220\n",
            "Epoch 00014: val_acc did not improve from 0.91040\n",
            "4688/4688 [==============================] - 1419s 303ms/step - loss: 0.1997 - acc: 0.9220 - val_loss: 0.3525 - val_acc: 0.8534 - lr: 0.0050\n",
            "Epoch 15/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.1984 - acc: 0.9238\n",
            "Epoch 00015: val_acc did not improve from 0.91040\n",
            "4688/4688 [==============================] - 1429s 305ms/step - loss: 0.1984 - acc: 0.9238 - val_loss: 0.3321 - val_acc: 0.8822 - lr: 0.0050\n",
            "Epoch 16/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.1947 - acc: 0.9251\n",
            "Epoch 00016: val_acc did not improve from 0.91040\n",
            "4688/4688 [==============================] - 1427s 304ms/step - loss: 0.1947 - acc: 0.9251 - val_loss: 0.6323 - val_acc: 0.7981 - lr: 0.0050\n",
            "Epoch 17/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.1922 - acc: 0.9269\n",
            "Epoch 00017: val_acc improved from 0.91040 to 0.91570, saving model to /content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5\n",
            "4688/4688 [==============================] - 1431s 305ms/step - loss: 0.1922 - acc: 0.9269 - val_loss: 0.2142 - val_acc: 0.9157 - lr: 0.0050\n",
            "Epoch 18/20\n",
            "4688/4688 [==============================] - ETA: 0s - loss: 0.1896 - acc: 0.9273\n",
            "Epoch 00018: val_acc did not improve from 0.91570\n",
            "4688/4688 [==============================] - 1436s 306ms/step - loss: 0.1896 - acc: 0.9273 - val_loss: 0.3575 - val_acc: 0.8569 - lr: 0.0050\n",
            "Epoch 19/20\n",
            "  81/4688 [..............................] - ETA: 23:10 - loss: 0.1902 - acc: 0.9244"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Predictions and create Submission CSV file"
      ],
      "metadata": {
        "id": "T303lGzO6xVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/gdrive/MyDrive/Histopath/Saved_models/6Layer_bestmodel_011722.h5')\n",
        "y_pred = pd.read_csv('/content/gdrive/MyDrive/Histopath/blank_sample_submission.csv')\n",
        "test_comp = '/content/gdrive/MyDrive/Histopath/test'\n",
        "y_pred['id'] = y_pred['id'].apply(append_ext)"
      ],
      "metadata": {
        "id": "Z0X1UBS5o_Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_files = pd.DataFrame({'file_name': os.listdir(test_comp)})\n",
        "# with open('/content/gdrive/MyDrive/Histopath/full_test_files', 'wb') as f:\n",
        "#   pickle.dump(test_files, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Histopath/Saved_data/full_test_files', 'rb') as f:\n",
        "  test_files = pickle.load(f)"
      ],
      "metadata": {
        "id": "rZIHm08wMlC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1/255.0)  \n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_files,\n",
        "    directory=test_comp,\n",
        "    x_col='file_name',\n",
        "    target_size=(96, 96),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcFq7lNbpapn",
        "outputId": "a78f8f61-8ee8-4c6d-cc11-0b0ac6e4ad74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57458 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_test = len(test_files)\n",
        "len_test"
      ],
      "metadata": {
        "id": "hC8mdSJxycED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f82cc08-08c1-4c4b-fea3-f2028a538bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57458"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_generator,\n",
        "                      steps=len_test,\n",
        "                      verbose=1)"
      ],
      "metadata": {
        "id": "Hti5H1Elsxam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514244a0-9c02-4ee0-8b1f-e844e46a5b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57458/57458 [==============================] - 11144s 194ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the prediction labels"
      ],
      "metadata": {
        "id": "_dY6zFyA6Chs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = preds[:, 0]\n",
        "preds = np.round(preds)\n",
        "preds.astype(int)"
      ],
      "metadata": {
        "id": "z-KhDlyYAol9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7cf615-e6e2-4241-dac2-759db823ddef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the prediction labels into the submission dataframe"
      ],
      "metadata": {
        "id": "Bsw2eSvQ6QTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_names = test_generator.filenames\n",
        "df_preds = pd.DataFrame({'id': test_file_names, 'label': preds})\n",
        "y_pred = y_pred.merge(df_preds, on='id')"
      ],
      "metadata": {
        "id": "cAu43Go4Bq8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get submission dataframe into correct format"
      ],
      "metadata": {
        "id": "T1bp68f7lT26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred['label'] = y_pred['label_y']\n",
        "y_pred.drop(['label_x'], axis=1, inplace=True)\n",
        "y_pred.drop(['label_y'], axis=1, inplace=True)\n",
        "y_pred['id'] = y_pred['id'].apply(remove_ext)"
      ],
      "metadata": {
        "id": "ZMnsdxJDFYvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.to_csv('/content/gdrive/MyDrive/Histopath/sample_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "bJLu8KQVHTak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get AUC"
      ],
      "metadata": {
        "id": "4UYCKIRHrksc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_generator = validation_datagen.flow_from_directory(\n",
        "    valid_comp,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=1,\n",
        "    class_mode='binary',\n",
        "    shuffle=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwPqoAVigRPj",
        "outputId": "d3764723-4efc-4342-bb3d-cec9e27b90df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_val_test = len(val_test_generator)\n",
        "\n",
        "val_test_pred = model.predict(val_test_generator, \n",
        "                         steps=len_val_test,\n",
        "                         verbose=1)[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlIbNP74Tg27",
        "outputId": "7444e049-8005-49e6-d15c-4fac5e83d080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 2117s 212ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_true = val_test_generator.classes\n",
        "val_test_pred = np.round(val_test_pred)\n",
        "val_test_pred.astype(int)"
      ],
      "metadata": {
        "id": "-7mIDdA_T04Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b2c21c-6a4b-4d1d-a2e3-fc86e587ae2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_val_test = np.equal(val_test_pred, val_test_true).sum() / len(val_test_pred)\n",
        "print('Validation accuracy: {:.3f}'.format(acc_val_test))"
      ],
      "metadata": {
        "id": "gMNeUgqlW3Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbbeade-af51-43b2-8ff2-aba58d482b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(val_test_true, val_test_pred)"
      ],
      "metadata": {
        "id": "uTpPxroJZrUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_val_test = auc(fpr, tpr)\n",
        "print('Validation AUC: {:.3f}'.format(auc_val_test))"
      ],
      "metadata": {
        "id": "e0E5wLBFiG95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb5ab15-88e8-473f-8052-316ba282c503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation AUC: 0.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SfTfLVlHq79M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eZGb3Orodx0t",
        "BjWXS8h_QJ5R",
        "fvwQgw1vROUg",
        "T303lGzO6xVi",
        "4UYCKIRHrksc"
      ],
      "name": "HCD model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLrzRRgbWZmXcfj3HLG4JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}